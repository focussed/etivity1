{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "lab2_data_preparation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "v9PovmdJzUQ7",
        "6McxNkv8zURR"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/focussed/etivity1/blob/master/lab2_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMv6yXsGzUO-",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2: Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcFJFhkKzUPE",
        "colab_type": "text"
      },
      "source": [
        "In the exploration of the *loans* dataset, we discovered a few issues, which need to be resolved before the dataset is ready for machine learning (ML). This exercise is typically referred as either *data preparation*, or *data preprocessing*, or *data munging*, or *data wrangling*. \n",
        "\n",
        "Here are the problems, we are already aware of:\n",
        "\n",
        "- There are missing values in some columns. We can estimate these values depending on the number of missing values and the expected importance of the column.\n",
        "\n",
        "- We observed that `ApplicantIncome` and `LoanAmount` seem to contain extreme values at either end. Although they might make intuitive sense, they should be treated appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nhiE8FUzUPH",
        "colab_type": "text"
      },
      "source": [
        "## A. Import Python Modules and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZSgcWrGzUPL",
        "colab_type": "code",
        "outputId": "2a0f39e3-d474-4d79-89c2-964577ebeeea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "url_train = 'https://raw.githubusercontent.com/focussed/etivity1/master/loans_train.csv'\n",
        "df = pd.read_csv(url_train)\n",
        "df.tail()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1281c8f50433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./loans_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./loans_train.csv' does not exist: b'./loans_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2dziC_zUPV",
        "colab_type": "text"
      },
      "source": [
        "Check for missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xRSN8mnzUPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGAYmLi7zUPj",
        "colab_type": "text"
      },
      "source": [
        "Before we start transforming the dataset let's make a copy of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqhMqluRzUPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_original = df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIYoQx-nzUPq",
        "colab_type": "text"
      },
      "source": [
        "## B. Process and Encode the Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4v8-CjPzUPs",
        "colab_type": "text"
      },
      "source": [
        "### Column `Loan_ID`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZFslT34zUPv",
        "colab_type": "text"
      },
      "source": [
        "Columns with ID numbers are unlikely to contain useful information and should be dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYMJGt2dzUPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop('Loan_ID', axis=1, inplace=True)\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqUSfwTzUP5",
        "colab_type": "text"
      },
      "source": [
        "### Encode Ordinal Categorical Attributes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqOzk0QqzUP7",
        "colab_type": "text"
      },
      "source": [
        "Note that there is only one categorical column, the column `Dependents`, which is **ordinal**. All other categorical columns (after deleting `Loan_ID`) are **nominal**.\n",
        "\n",
        "Let's first encode `Dependents`. While `Dependents` does look like a numerical column, it is not, because one of the possible values '3+' which is not a number. Still, there is an inherent order among the possible values in this column, i.e. 0 < 1 < 2 < 3+. Thus, we can encode it by replacing '3+' by 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4-ph0TozUP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a mapper\n",
        "\n",
        "scale_mapper = {\n",
        "    \"0\": 0,\n",
        "    \"1\": 1,\n",
        "    \"2\": 2,\n",
        "    \"3+\": 3\n",
        "}\n",
        "\n",
        "df['Dependents'] = df['Dependents'].replace(scale_mapper)\n",
        "\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmNMk-jozUQB",
        "colab_type": "text"
      },
      "source": [
        "### One-Hot Encode All Other Categorical Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaq8btk8zUQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first one-hot encode the categorical columns with NaNs\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Self_Employed'], \n",
        "                        dummy_na=True, \n",
        "                        drop_first=True)\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk2mo_oDzUQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now one-hot encode all other categorical columns\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cniZz6OzUQL",
        "colab_type": "text"
      },
      "source": [
        "## C. Treatment of Missing Values in Numerical Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFdf8Q0qzUQN",
        "colab_type": "text"
      },
      "source": [
        "### Check for Missing Values\n",
        "\n",
        "It is often useful to impute missing values as many ML algorithms do not work with missing data and even if they do, imputing these values often helps to build a more accurate predictive model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyi8ef_QzUQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the number of nulls/NaNs in the dataset\n",
        "\n",
        "df.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVGxo-YizUQV",
        "colab_type": "text"
      },
      "source": [
        "Note that missing values may not always be NaNs. For instance, if `Loan_Amount_Term` had zeros, would they make sense or should they be considered missing? That is, we need to understand the dataset well in order to identify which values are missing.\n",
        "\n",
        "Note also that no column has more than 25% missing values. Thus, we do not need to consider dropping whole columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi-KNw5BzUQX",
        "colab_type": "text"
      },
      "source": [
        "### Impute Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRc1p1kGzUQZ",
        "colab_type": "text"
      },
      "source": [
        "There are numerous ways to impute (i.e., fill in) the missing values in a numerical column, e.g. column `LoanAmount` – the simplest being replacement by mean, which can be done with the following code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY_6HrjszUQa",
        "colab_type": "text"
      },
      "source": [
        "```pyton\n",
        "df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsePg-VnzUQb",
        "colab_type": "text"
      },
      "source": [
        "Imputing missing values with the mean (alternatively, the media, the most frequent, or a constant) may introduce bias in the dataset. A better method is to build an ML model (typically, kNN) to predict `LoanAmount` on the basis of other columns.\n",
        "\n",
        "Since, the purpose now is to practice data munging prior applying ML algorithms, we can take an approach, which lies somewhere in between these 2 extremes. We may hypothesise that attributes `Education` and `Self_Employed` combined can give a good estimate of `LoanAmount`.\n",
        "\n",
        "First, let us look at the following boxplots to see if a trend exists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnSd543tzUQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.boxplot(column='LoanAmount', by=['Education_Not Graduate','Self_Employed_Yes'], rot=45)\n",
        "plt.title(\"\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxCYPDLzUQg",
        "colab_type": "text"
      },
      "source": [
        "We can see some variation in the median of `LoanAmount` for each group and these medians can be used for imputing the missing values in each group. Next, we create a pivot table, which contains the median values for all four groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ap-sKY5zUQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ptable = df.pivot_table(values='LoanAmount', \n",
        "                        index='Self_Employed_Yes',\n",
        "                        columns='Education_Not Graduate',  \n",
        "                        aggfunc=np.median)\n",
        "ptable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58CZpoRXzUQl",
        "colab_type": "text"
      },
      "source": [
        "Finally, we define a function, which returns the values in the pivot table and apply it to fill the missing values of `LoanAmount`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtKo5c2zUQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function to return an element of the pivot table\n",
        "def get_element(x):\n",
        "    return ptable.loc[x['Self_Employed_Yes'], x['Education_Not Graduate']]\n",
        "\n",
        "# Replace missing values\n",
        "df['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(get_element, axis=1), inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSt21KDbzUQr",
        "colab_type": "text"
      },
      "source": [
        "Let's take a simplified approach for `Dependents`, `Loan_Amount_Term` and `Credit_History` and fill in the missing values in these columns with the median. Note that the median is likely a better choice than the mean for these columns because the means are not likely to be realistic values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VKSLsNSzUQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Dependents'].fillna(df['Dependents'].median(), inplace=True)\n",
        "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].median(), inplace=True)\n",
        "df['Credit_History'].fillna(df['Credit_History'].median(), inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezyPWFwQzUQx",
        "colab_type": "text"
      },
      "source": [
        "## D. Rescaling Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_6-ocJnzUQx",
        "colab_type": "text"
      },
      "source": [
        "Many ML algorithms assume that all numerical features are on the same scale. Two standard techniques in the Python module `scikit-learn` for achieving this are:\n",
        "* MinMaxScaler - rescales a column to the interval [0,1]\n",
        "* StandardScaler - rescales a numerical column so that it has mean 0 and standard deviation 1.\n",
        "\n",
        "Here we use a third technique, called `RobustScaler` to `LoanAmount` and `ApplicantIncome` which does a better job in the presence of outliers (we observed outliers in columns `LoanAmount` and `ApplicantIncome`). `RobustScaler` transforms each value in a column by subtracting the median from it and then dividing by the interquartile range.\n",
        "\n",
        "We will deal with `CoapplicantIncome` and `Loan_Amount_Term` later. Note that all other attributes are binary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi5Pu47JzUQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "robust_scaler = RobustScaler()\n",
        "df[['ApplicantIncome',\n",
        "    'LoanAmount']] = robust_scaler.fit_transform(df[['ApplicantIncome',\n",
        "                                                     'LoanAmount']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkWmCRv1zUQ1",
        "colab_type": "text"
      },
      "source": [
        "To bring column `Dependents` to the interval $[0,1]$ we can simply divide it by 3, as it is an ordinal columns with four possible values: 0, 1, 2 and 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBXonf0rzUQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Dependents'] = df['Dependents']/3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kQsD32MzUQ5",
        "colab_type": "text"
      },
      "source": [
        "## E. Treatment of Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9PovmdJzUQ7",
        "colab_type": "text"
      },
      "source": [
        "#### `ApplicantIncome` and `LoanAmount`\n",
        "\n",
        "In the previous lab exercise, we observed that `ApplicantIncome` and `LoanAmount` contain some extreme values. Let's take a look at the box plots of `ApplicantIncome`,`LoanAmount` to verify that they have outliers. We will deal with `CoapplicantIncome` and `Loan_Amount_Term` later. All other columns do not have outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2p9gqZzUQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.boxplot(column=['ApplicantIncome','LoanAmount'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ6jF-RAzUQ-",
        "colab_type": "text"
      },
      "source": [
        "Let's also take a look at their histograms after we have rescaled them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA8-lDF6zURB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create figure with two subplots\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "\n",
        "# Plot ApplicantIncome\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.set_title(\"Histogram of ApplicantIncome\")\n",
        "ax1.set_xlabel('ApplicantIncome')\n",
        "ax1.set_ylabel('Number of Applicants')\n",
        "df['ApplicantIncome'].hist(bins=20)\n",
        "\n",
        "# Plot LoanAmount\n",
        "ax3 = fig.add_subplot(1, 2, 2)\n",
        "ax3.set_title(\"Histogram of LoanAmount\")\n",
        "ax3.set_xlabel('LoanAmount')\n",
        "ax3.set_ylabel('Number of Applicants')\n",
        "df['LoanAmount'].hist(bins=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj2VuUirzURE",
        "colab_type": "text"
      },
      "source": [
        "Both histograms are skewed to the left, which is not ideal. Many ML algorithms work best when the features are not skewed to either side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxxQ6CNVzURF",
        "colab_type": "text"
      },
      "source": [
        "Finally, before treating the outliers, let's find the minimum values in these columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5qqv41UzURG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['ApplicantIncome', 'LoanAmount']].min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ImijzezURJ",
        "colab_type": "text"
      },
      "source": [
        "Since the extreme values are practically possible in both columns, i.e. some people might have large income and also apply for high-value loans, instead of ignoring the data rows with extreme values we can transform them to diminish the negative impact of outliers to ML models. A typical way to do this is to apply a log transformation. However, we do have negative values in both columns after rescaling and we need to add a constant to all values in order to shift them to a positive range before applying a log transformation. \n",
        "\n",
        "Here we are have chosen to shift the values to the interval $[1, +\\infty]$ before applying a log transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBq5nywszURK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use a log transformation to decrease the impact of outliers\n",
        "df['ApplicantIncome'] = np.log(df['ApplicantIncome']+2.26)\n",
        "df['LoanAmount'] = np.log(df['LoanAmount']+2.85)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDoacWTazURM",
        "colab_type": "text"
      },
      "source": [
        "Let's plot again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lrcR_82zURN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create figure with two subplots\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "\n",
        "# Plot ApplicantIncome\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.set_title(\"Histogram of ApplicantIncome Transformed\")\n",
        "ax1.set_xlabel('ApplicantIncome')\n",
        "ax1.set_ylabel('Number of Applicants')\n",
        "df['ApplicantIncome'].hist(bins=20)\n",
        "\n",
        "# Plot LoanAmount\n",
        "ax3 = fig.add_subplot(1, 2, 2)\n",
        "ax3.set_title(\"Histogram of LoanAmount Transformed\")\n",
        "ax3.set_xlabel('LoanAmount')\n",
        "ax3.set_ylabel('Number of Applicants')\n",
        "df['LoanAmount'].hist(bins=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVLkTeh3zURQ",
        "colab_type": "text"
      },
      "source": [
        "The histograms show that the transformed features are less skewed and have distributions closer to normal than the original features `LoanAmount` and `ApplicantIncome`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6McxNkv8zURR",
        "colab_type": "text"
      },
      "source": [
        "#### `Loan_Amount_Term`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Qnc8F2zURR",
        "colab_type": "text"
      },
      "source": [
        "Let's examine the distribution of values in `Loan_Amount_Term`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epTcDldKzURS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Loan_Amount_Term'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytiTeLw8zURV",
        "colab_type": "text"
      },
      "source": [
        "Note that `Loan_Amount_Term` is rather an ordinal than a continuous numerical feature with more than 80% of its values being 360.0. None of its values appear to be actual outliers. To bring it to the interval $[0,1]$ we can divide all values by 480 (effectively applying MinMaxScaler to it). If we applied RobustScaler and/or log transformation to it instead then it would become even more skewed to the right (you may verify this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_dHGGhPzURW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Loan_Amount_Term'] = df['Loan_Amount_Term']/480\n",
        "df['Loan_Amount_Term'].hist(bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkTdTlltzURZ",
        "colab_type": "text"
      },
      "source": [
        "## F. Creation of Derived Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY1xhTCgzURa",
        "colab_type": "text"
      },
      "source": [
        "Column `CoapplicantIncome` contains many zeros which can be interpreted as `there is no co-applicant with income greater than 0`. The zeros are *missing values* which should not be filled in because they are missing for a reason. On the other hand, they skew the feature towards 0 and this cannot be easily fixed with a log transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8rldl7bzURl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['CoapplicantIncome'].hist(bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6KhJdDzURo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.log(df['CoapplicantIncome']+1).hist(bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEbhpEIGzURq",
        "colab_type": "text"
      },
      "source": [
        "What we can do in this case instead is:\n",
        "* Create a new feature `TotalIncome` as the sum of the *original* `ApplicantIncome` and `CoapplicantIncome`\n",
        "* Drop column `CoapplicantIncome` because it can be derived from `ApplicantIncome` and `CoapplicantIncome`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGC2QVGKzURr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create TotalIncome column and apply a log transformation\n",
        "df['TotalIncome'] = df_original['ApplicantIncome'] + df_original['CoapplicantIncome']\n",
        "df[['TotalIncome']] = robust_scaler.fit_transform(df[['TotalIncome']])\n",
        "df[['TotalIncome']].min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al-j7OATzURu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['TotalIncome'] = np.log(df['TotalIncome']+2.19)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRh9SsZdzURx",
        "colab_type": "text"
      },
      "source": [
        "Let's plot `TotalIncome`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liwBBaQ-zUR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['TotalIncome'].hist(bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iHU4RECzUR3",
        "colab_type": "text"
      },
      "source": [
        "Let's drop `CoapplicantIncome`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2w8sed1zUR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop('CoapplicantIncome', axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D13AF_VTzUSK",
        "colab_type": "text"
      },
      "source": [
        "# G. Save the Prepared Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgA3q_9DzUSL",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's save the prepared dataset as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ptuSAqzUSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('loans_train_prepared.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}