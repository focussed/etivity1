{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "etivity5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/focussed/etivity1/blob/master/Etivity5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWXNbReuh2tq",
        "colab_type": "text"
      },
      "source": [
        "#**Etivity5: Regression and Dimensionality Reduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJ-Wo7ih2tr",
        "colab_type": "text"
      },
      "source": [
        "## Summary of work done\n",
        "This etivity5 is based on the lab 5 example notebook and is presented after a number of trials and error on various reduction methods, research and regression models were tried on the white wine dataset.\n",
        "\n",
        "The main additions to this notebook are:\n",
        "\n",
        "\n",
        "*   Moving the training split from 80%/20% to 60%/40%\n",
        "*   Increasing the number of metrics from three to six\n",
        "*   Analysis of low variance features.\n",
        "*   Investigation of Fast ICA, VarianceThreshold and LocallyLinearEmbedding dimension reduction methods.  (LocallyLinearEmbedding results were not included here).\n",
        "*   Investigation of the K nearest Neighbour regression model with various different vales for k.\n",
        "\n",
        "it was found that the Random Forest Regression algorithm was the most accurate regression model.  The k nearest neighbour model had the lowest mean square error for k-11, but it was less accurate than the linear regression and random forest models.\n",
        "\n",
        "### Algorithms considered\n",
        "In this etivity, we examine the performance of regression a dimensional reduction models to perform data pattern matching.  The models being considered in this etivity are:\n",
        "\n",
        "1.   Linear Regression\n",
        "2.   Random Forest Regression\n",
        "3.   K Nearest Neighbour regression\n",
        "\n",
        "### Dataset\n",
        "The dataset under treatment for this etivity is `winequality-white` which is a complimentary dataset of the Lab example for etivity5.\n",
        "\n",
        "## Task 2\n",
        "### Random Forest Regression\n",
        "The grid run for determining the best parameters takes a long time to run.  The results of this CVGrid are as follows:\n",
        "\n",
        "Best CV score = 0.389:\n",
        "Best parameters:  {'reduce_dim': RFE(estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
        "                  gamma='auto', kernel='linear', max_iter=-1, shrinking=True,\n",
        "                  tol=0.001, verbose=False),\n",
        "    n_features_to_select=11, step=1, verbose=0), 'reduce_dim__n_features_to_select': 11, 'regresson__max_depth': 8}\n",
        "\n",
        "## Linear regression\n",
        "The CV Grid test run found the best parameters for the model were:\n",
        "Best CV score = 0.277:\n",
        "Best parameters:  {'reduce_dim': PCA(copy=True, iterated_power=7, n_components=11, random_state=None,\n",
        "    svd_solver='auto', tol=0.0, whiten=False), 'reduce_dim__n_components': 11, 'regresson__normalize': False}\n",
        "\n",
        "Of the two models, the random forest regression model consistently performs better with the winequality-white dataset in predicting the quality from the data.\n",
        "\n",
        "Three other metrics to help evaluate performance of the models and include these below in the plot.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT7PBgyvh2ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# imports necessary for dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn import svm\n",
        "\n",
        "# regression algorithms\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# metrics for evaluating regression models\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etm3rh4Eh2tw",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH7WBqb7h2tx",
        "colab_type": "text"
      },
      "source": [
        "We start with a brief EDA to check for missing values and outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfjOb7q3h2tx",
        "colab_type": "code",
        "outputId": "191e7694-8cf9-4e88-d1f3-ab85ce25ff78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/focussed/etivity1/master/winequality-white.csv'\n",
        "df = pd.read_csv(url)\n",
        "print('(number of examples, number of attributes): ', df.shape)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(number of examples, number of attributes):  (4898, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WFgqF5th2t1",
        "colab_type": "code",
        "outputId": "e7363704-9c1e-4cf5-9f90-89bd1167f76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "df['quality'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    4898.000000\n",
              "mean        5.877909\n",
              "std         0.885639\n",
              "min         3.000000\n",
              "25%         5.000000\n",
              "50%         6.000000\n",
              "75%         6.000000\n",
              "max         9.000000\n",
              "Name: quality, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzPlWC4-h2t4",
        "colab_type": "text"
      },
      "source": [
        "First of all let's check for missing values and outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zmq_cFVh2t5",
        "colab_type": "code",
        "outputId": "f10c5e7f-7aff-4f90-8fc5-e5c0bac89832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fixed acidity           0\n",
              "volatile acidity        0\n",
              "citric acid             0\n",
              "residual sugar          0\n",
              "chlorides               0\n",
              "free sulfur dioxide     0\n",
              "total sulfur dioxide    0\n",
              "density                 0\n",
              "pH                      0\n",
              "sulphates               0\n",
              "alcohol                 0\n",
              "quality                 0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThFx8nOdh2t8",
        "colab_type": "text"
      },
      "source": [
        "There are no missing values. Let's check the boxplots of all columns except `quality` for outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CshK3xe5h2t8",
        "colab_type": "code",
        "outputId": "212866c3-bd76-4498-e796-8fdac9cc08e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "df.boxplot(figsize=(20,5))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNIZ1Cebh2t_",
        "colab_type": "text"
      },
      "source": [
        "The columns `residual sugar`, `free sulfur dioxide` and `total sulfur dioxide` appear to have most prominent outliers. We will apply log transformation followed by `RobustScaler` to them. To all other columns (except `quality`) we will apply `StandardScaler`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrMSb96-NxSu",
        "colab_type": "text"
      },
      "source": [
        "### Check the variance and other stats of all columns\n",
        "We may be able to discard some columns based on a low variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcoGD3-KODBF",
        "colab_type": "code",
        "outputId": "fa01e87a-3634-440f-9bde-681af4c236f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "stats=pd.DataFrame()\n",
        "stats[\"min\"]=df.min()\n",
        "stats[\"mean\"]=df.mean()\n",
        "stats[\"median\"]=df.median()\n",
        "stats[\"max\"]=df.max()\n",
        "stats[\"Std.Dev\"]=df.std()\n",
        "stats[\"Variance\"]=df.var()\n",
        "stats.T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>3.800000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.987110</td>\n",
              "      <td>2.720000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6.854788</td>\n",
              "      <td>0.278241</td>\n",
              "      <td>0.334192</td>\n",
              "      <td>6.391415</td>\n",
              "      <td>0.045772</td>\n",
              "      <td>35.308085</td>\n",
              "      <td>138.360657</td>\n",
              "      <td>0.994027</td>\n",
              "      <td>3.188267</td>\n",
              "      <td>0.489847</td>\n",
              "      <td>10.514267</td>\n",
              "      <td>5.877909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median</th>\n",
              "      <td>6.800000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>5.200000</td>\n",
              "      <td>0.043000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>0.993740</td>\n",
              "      <td>3.180000</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>10.400000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.200000</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>1.660000</td>\n",
              "      <td>65.800000</td>\n",
              "      <td>0.346000</td>\n",
              "      <td>289.000000</td>\n",
              "      <td>440.000000</td>\n",
              "      <td>1.038980</td>\n",
              "      <td>3.820000</td>\n",
              "      <td>1.080000</td>\n",
              "      <td>14.200000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Std.Dev</th>\n",
              "      <td>0.843868</td>\n",
              "      <td>0.100795</td>\n",
              "      <td>0.121020</td>\n",
              "      <td>5.072058</td>\n",
              "      <td>0.021848</td>\n",
              "      <td>17.007137</td>\n",
              "      <td>42.498065</td>\n",
              "      <td>0.002991</td>\n",
              "      <td>0.151001</td>\n",
              "      <td>0.114126</td>\n",
              "      <td>1.230621</td>\n",
              "      <td>0.885639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variance</th>\n",
              "      <td>0.712114</td>\n",
              "      <td>0.010160</td>\n",
              "      <td>0.014646</td>\n",
              "      <td>25.725770</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>289.242720</td>\n",
              "      <td>1806.085491</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.022801</td>\n",
              "      <td>0.013025</td>\n",
              "      <td>1.514427</td>\n",
              "      <td>0.784356</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          fixed acidity  volatile acidity  ...    alcohol   quality\n",
              "min            3.800000          0.080000  ...   8.000000  3.000000\n",
              "mean           6.854788          0.278241  ...  10.514267  5.877909\n",
              "median         6.800000          0.260000  ...  10.400000  6.000000\n",
              "max           14.200000          1.100000  ...  14.200000  9.000000\n",
              "Std.Dev        0.843868          0.100795  ...   1.230621  0.885639\n",
              "Variance       0.712114          0.010160  ...   1.514427  0.784356\n",
              "\n",
              "[6 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipPnOfqKRLcU",
        "colab_type": "text"
      },
      "source": [
        "Looks like we can drop volatile acidity, citric acid, chlorides, density and sulphates.  They all have low variance values.\n",
        "\n",
        "This does not have to be done manually as the ThresholdVariance() feature reduction method will do this with respect to the tolerances shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oowna8dh2uA",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D5cLyyOh2uB",
        "colab_type": "text"
      },
      "source": [
        "First, we need to prepare two lists of column names. The list `names_outliers` contains the names of the two columns to which we will apply log transformation followed by `RobustScaler`. The list `names_no_outliers` contains the names of all other columns (except `quality`) to which we will apply `StandardScaler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qPUKtUh2uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store a list with the names of all predictors\n",
        "names_all = [c for c in df if c not in ['quality']]\n",
        "\n",
        "# define column groups with the same data preparation\n",
        "names_outliers = ['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide']\n",
        "names_no_outliers = list(set(names_all) - set(names_outliers))\n",
        "\n",
        "# We can take out the low variance featues manually if we wish\n",
        "#names_low_variances = ['volatile acidity', 'citric acid', 'chlorides', 'density', 'sulphates']\n",
        "#names_no_outliers = list(set(names_all) - set(names_outliers) - set(names_low_variances))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHo1-pbNh2uI",
        "colab_type": "text"
      },
      "source": [
        "After splitting a dataset into a training and test sets, the names of the columns are lost. This is the reason, we stored the names of the columns in lists above. We will use the following class in the preprocessing pipeline to put the names of the columns back. We need this to easily apply the different preparation strategies to the two groups of columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u-D2Pi0h2uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddColumnNames(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return pd.DataFrame(data=X, columns=self.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvGEOKGDh2uO",
        "colab_type": "text"
      },
      "source": [
        "Then we need another class to be able to select a particular group of columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lW0Ozuvh2uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        assert isinstance(X, pd.DataFrame)\n",
        "        return X[self.columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud9mjr-hh2uS",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can build the preprocessing pipeline. It first adds the column names back to a set of examples (that can be either a training, or a validation, or test set). Then it applies the two different data preparation strategies to the two groups of columns and unites them with `FeatureUnion`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es8Js3TUh2uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess_pipeline = make_pipeline(\n",
        "    AddColumnNames(columns=names_all),\n",
        "    FeatureUnion(transformer_list=[\n",
        "        (\"outlier_columns\", make_pipeline(\n",
        "            ColumnSelector(columns=names_outliers),\n",
        "            FunctionTransformer(np.log, validate=True),\n",
        "            RobustScaler()\n",
        "        )),\n",
        "        (\"no_outlier_columns\", make_pipeline(\n",
        "            ColumnSelector(columns=names_no_outliers),\n",
        "            StandardScaler()\n",
        "        ))\n",
        "    ])\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh9g9z-1h2ua",
        "colab_type": "text"
      },
      "source": [
        "Now we can separate the columns into *target* and *predictors* and split the dataset into a training and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLpkWBrkh2ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df['quality']\n",
        "X = df.drop('quality', axis=1).values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk4cWznch2ud",
        "colab_type": "text"
      },
      "source": [
        "Although we treat `quality` as a numerical attribute, it is in fact ordinal, and we can still do a stratified split to ensure that the distribution of wine qualities is the same in both the training and the test sets.\n",
        "\n",
        "Note that after the split into a training and test sets, X_train and X_test are numpy arrays and no longer have column names. That's why we needed the class above to put the names of columns back in the preprocessing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCH3Hsygh2ue",
        "colab_type": "text"
      },
      "source": [
        "## Grid Search for Best Parameters and Best Dimensionality Reduction Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U820GPfTh2ue",
        "colab_type": "text"
      },
      "source": [
        "Next, we train `RandomForestRegressor` on the training set with a range of possible parameters in order to find the best parameters by cross-validation. To do this we will build another [main] pipeline which includes the preprocessing pipeline and `RandomForestRegressor`. We also add an element for dimensionality reduction after the preprocessing pipeline.\n",
        "\n",
        "Here we will attempt three different dimensionality reduction methods and we will let the grid search pick the best one. These are:\n",
        "\n",
        "- Principal Component Analysis (PCA)\n",
        "- Recursive Feature Elimination (RFE) with estimator `svm.SVR`\n",
        "- Recursive Feature Elimination (RFE) with estimator `LinearRegression`\n",
        "\n",
        "Note that RFE is using regression algorithms for selecting the best features. These regression algorithms can be different from the regression algorithm at the end of the main pipeline.\n",
        "\n",
        "The main pipeline will take care for separately preprocessing the training and validation sets after the training set is further split into training and validation sets in the process of cross-validation. It also applies the dimensionality reduction method separately to the two sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbkHmn6yh2uf",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-7mgHWth2uf",
        "colab_type": "text"
      },
      "source": [
        "The pipeline contains a placeholder for the dimensionality reduction method. We will treat the method as a parameter and let the grid search pick the best of the three methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCZzz0jDh2uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline(steps=[('preprocess', preprocess_pipeline), \n",
        "                       ('reduce_dim', 'passthrough'),\n",
        "                       ('regresson', RandomForestRegressor(n_estimators=10))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VigCby0h2ui",
        "colab_type": "text"
      },
      "source": [
        "We limit the parameter grid to a few options for the `max_depth` parameter of `RandomForestRegressor` and to three alternative values for the number of selected features by the dimensionality reduction method. More parameters and values can be explored. Here we limit the options to make sure the grid search does not take too long to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyEXEsyXh2uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_FEATURES_OPTIONS = [2, 6, 11]\n",
        "MAX_DEPTH_OPTIONS = [2, 4, 6, 8]\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'reduce_dim': [PCA(iterated_power=7)],\n",
        "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
        "        'regresson__max_depth': MAX_DEPTH_OPTIONS\n",
        "    },\n",
        "    {\n",
        "        'reduce_dim': [RFE(svm.SVR(kernel='linear', gamma='auto')),RFE(LinearRegression())],\n",
        "        'reduce_dim__n_features_to_select': N_FEATURES_OPTIONS,\n",
        "        'regresson__max_depth': MAX_DEPTH_OPTIONS\n",
        "    }  \n",
        "]\n",
        "\n",
        "search = GridSearchCV(pipe, param_grid, cv=10, refit=True, n_jobs=2)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best CV score = %0.3f:\" % search.best_score_)\n",
        "print(\"Best parameters: \", search.best_params_)\n",
        "\n",
        "# store the best params and best model for later use\n",
        "RF_best_params = search.best_params_\n",
        "RF_best_model = search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvOot4cth2um",
        "colab_type": "text"
      },
      "source": [
        "We explicitly assign the `False` to the parameter `iid` of `GridSearchCV` to avoid a deprecation warning. The parameter `refit=True` makes the `GridSearchCV` train a `RandomForestRegressor` model on the **whole training set** with the best parameters and the best dimensionality reduction method found. This best model can then be accessed via the `.best_estimator_` attribute of the `GridSearchCV`.\n",
        "\n",
        "Let's repeat the same experiment but with `LinearRegression` for training a regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdAthRhSj6Ld",
        "colab_type": "text"
      },
      "source": [
        "## Task 3\n",
        "Here we add two new methods for the Random Forest, FAST ICA and variance threshold to reduce parameters based on features with low variances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMbPONwCh7XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline(steps=[('preprocess', preprocess_pipeline), \n",
        "                       ('reduce_dim', 'passthrough'),\n",
        "                       ('regresson', RandomForestRegressor(n_estimators=10))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjJlysfYeoVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE, VarianceThreshold\n",
        "from sklearn.decomposition import FactorAnalysis, FastICA\n",
        "\n",
        "N_FEATURES_OPTIONS = [2, 6, 11]\n",
        "MAX_DEPTH_OPTIONS = [2, 4, 6, 8]\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'reduce_dim': [VarianceThreshold()],\n",
        "        'reduce_dim__threshold': [0.005, 0.05, 0.5],\n",
        "        'regresson__max_depth': MAX_DEPTH_OPTIONS\n",
        "    },\n",
        "    {\n",
        "        'reduce_dim': [FastICA()],\n",
        "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
        "        'regresson__max_depth': MAX_DEPTH_OPTIONS\n",
        "    }\n",
        "]\n",
        "\n",
        "search = GridSearchCV(pipe, param_grid, cv=10, refit=True, n_jobs=2)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best CV score = %0.3f:\" % search.best_score_)\n",
        "print(\"Best parameters: \", search.best_params_)\n",
        "\n",
        "# store the best params and best model for later use\n",
        "RF_best_params = search.best_params_\n",
        "RF_best_model = search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5bnPprdh2un",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dlZO1Qoh2uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline(steps=[('preprocess', preprocess_pipeline), \n",
        "                       ('reduce_dim', 'passthrough'),\n",
        "                       ('regresson', LinearRegression())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj4aIL-Eh2ur",
        "colab_type": "text"
      },
      "source": [
        "Again we limit the parameter grid to one parameter of `LinearRegression` and three alternative values for the number of selected features to make sure the grid search does not take too long to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQzeppvch2ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_FEATURES_OPTIONS = [2, 6, 11]\n",
        "NORMALIZE_OPTIONS = [False, True]\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'reduce_dim': [PCA(iterated_power=7)],\n",
        "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
        "        'regresson__normalize': NORMALIZE_OPTIONS\n",
        "    },\n",
        "    {\n",
        "        'reduce_dim': [RFE(svm.SVR(kernel='linear', gamma='auto')),RFE(LinearRegression())],\n",
        "        'reduce_dim__n_features_to_select': N_FEATURES_OPTIONS,\n",
        "        'regresson__normalize': NORMALIZE_OPTIONS\n",
        "    }  \n",
        "]\n",
        "\n",
        "search = GridSearchCV(pipe, param_grid, n_jobs=2, cv=10, refit=True)\n",
        "search.fit(X_train, y_train)\n",
        "print(\"Best CV score = %0.3f:\" % search.best_score_)\n",
        "print(\"Best parameters: \", search.best_params_)\n",
        "\n",
        "# store the best params and best model for later use\n",
        "LR_best_params = search.best_params_\n",
        "LR_best_model = search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw-UAUqQzfCm",
        "colab_type": "text"
      },
      "source": [
        "## Task 3\n",
        "Here we add two new methods for the Linear Regression model, FAST ICA and variance threshold to reduce parameters based on features with low variances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1erDUlz0k_Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline(steps=[('preprocess', preprocess_pipeline), \n",
        "                       ('reduce_dim', 'passthrough'),\n",
        "                       ('regresson', LinearRegression())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8MYZ84ku1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE, VarianceThreshold\n",
        "from sklearn.decomposition import FactorAnalysis, FastICA\n",
        "\n",
        "N_FEATURES_OPTIONS = [2, 6, 11]\n",
        "NORMALIZE_OPTIONS = [False, True]\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'reduce_dim': [VarianceThreshold()],\n",
        "        'reduce_dim__threshold': [0.005, 0.05, 0.5],\n",
        "        'regresson__normalize': NORMALIZE_OPTIONS\n",
        "    },\n",
        "    {\n",
        "        'reduce_dim': [FastICA()],\n",
        "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
        "        'regresson__normalize': NORMALIZE_OPTIONS\n",
        "    }  \n",
        "]\n",
        "\n",
        "search = GridSearchCV(pipe, param_grid, n_jobs=2, cv=10, refit=True)\n",
        "search.fit(X_train, y_train)\n",
        "print(\"Best CV score = %0.3f:\" % search.best_score_)\n",
        "print(\"Best parameters: \", search.best_params_)\n",
        "\n",
        "# store the best params and best model for later use\n",
        "LR_best_params = search.best_params_\n",
        "LR_best_model = search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oKOnKalzmin",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion for task 3\n",
        "The new methods introduced for feature reduction has not improved the accuracy of the model.  In both cases they as the same or slightly less accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zG2r5YmlW10",
        "colab_type": "text"
      },
      "source": [
        "# Task 4\n",
        "## K Nearest Neighbour Reression Model\n",
        "The k nearest neighbour regression model is the one chosen as a third regression model.  It was used as a classifier model in Etivity 4 with good success, so we will try the regression model here.\n",
        "\n",
        "k defaults to 5, but we will vary k and see how the mean square error varies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE-FHqpdZj2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "mseforK = [] \n",
        "for K in range(20):\n",
        "    K = K+1\n",
        "    model = KNeighborsRegressor(n_neighbors = K)\n",
        "\n",
        "    model.fit(X_train, y_train)  \n",
        "    pred_y=model.predict(X_test)\n",
        "    localmse = (mean_squared_error(y_test, pred_y))\n",
        "    mseforK.append(localmse)\n",
        "    print('k =' , K , ', mse =', localmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxz21ustnQML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "curve = pd.DataFrame(mseforK)\n",
        "plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-SoVwfGmHoH",
        "colab_type": "text"
      },
      "source": [
        "Typically, in runs (see graph above), k has lowest mse for values between 11 and 14.  Will do a run here with 12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qqCi5rj4eKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "KNN_model = KNeighborsRegressor(n_neighbors=12, weights='uniform', algorithm='auto', leaf_size=30)\n",
        "KNN_model.fit(X_train, y_train)\n",
        "pred = KNN_model.predict(X_test)\n",
        "score = KNN_model.score(X_test, pred)\n",
        "print('KNN. R2 score :', score)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46kY7BqMh2uv",
        "colab_type": "text"
      },
      "source": [
        "## Compare Regression Models on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXF7ihYvh2uw",
        "colab_type": "text"
      },
      "source": [
        "Now we can evaluate the best models found by the grid search on the test dataset and compare their metrics:\n",
        "\n",
        "- mean squared error (MSE)\n",
        "- mean absolute error (MAE)\n",
        "- 1-relative squared error (R2)\n",
        "- mean squared log error (MSLE)\n",
        "- mefian absolute error (MAE)\n",
        "- explained variance (EV)\n",
        "\n",
        "to choose the better regressor for our problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB7-bc2Gh2uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model - a trained regression model\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def evaluate_model(X_test, y_test, model):\n",
        "    \n",
        "    # compute predictiond for the test set\n",
        "    _predicted_values = model.predict(X_test)\n",
        "        \n",
        "    # compute metrics\n",
        "    _mse = mean_squared_error(y_test, _predicted_values)\n",
        "    _mae = mean_absolute_error(y_test, _predicted_values)\n",
        "    _r2 = r2_score(y_test, _predicted_values)\n",
        "    _explained_variance=metrics.explained_variance_score(y_test, _predicted_values)  \n",
        "    _mean_squared_log_error=metrics.mean_squared_log_error(y_test, _predicted_values)\n",
        "    _median_absolute_error=metrics.median_absolute_error(y_test, _predicted_values)\n",
        "    return _mse, _mae, _r2, _explained_variance, _mean_squared_log_error, _median_absolute_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bucdy7nNh2uz",
        "colab_type": "text"
      },
      "source": [
        "We will use the function above to evaluate the best Random Forest and Linear Regression models found by the grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1FSsKuLh2u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RF_mse, RF_mae, RF_r2, RF_explained_variance, RF_mean_sle, RF_median_ae = evaluate_model(X_test, y_test, RF_best_model)\n",
        "LR_mse, LR_mae, LR_r2, LR_explained_variance, LR_mean_sle, LR_median_ae = evaluate_model(X_test, y_test, LR_best_model)\n",
        "KNN_mse, KNN_mae, KNN_r2, KNN_explained_variance, KNN_mean_sle, KNN_median_ae = evaluate_model(X_test, y_test, KNN_model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuAYp2Vrh2u2",
        "colab_type": "text"
      },
      "source": [
        "We will use a Pandas bar plot to compare the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr7CJFh4h2u3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RF_metrics = np.array([RF_mse, RF_mae, RF_r2, RF_explained_variance, RF_mean_sle, RF_median_ae, ])\n",
        "LR_metrics = np.array([LR_mse, LR_mae, LR_r2, LR_explained_variance, LR_mean_sle, LR_median_ae])\n",
        "KNN_metrics = np.array([KNN_mse, KNN_mae, KNN_r2, KNN_explained_variance, KNN_mean_sle, KNN_median_ae])\n",
        "\n",
        "index = ['MSE', 'MAE', 'R2', 'ExplVar', 'mean_sle', 'median_ae']\n",
        "df_metrics = pd.DataFrame({'Random Forest': RF_metrics, 'Linear Regression': LR_metrics, 'K Nearest Neighbour = 5 Regression': KNN_metrics}, index=index)\n",
        "df_metrics.plot.bar(rot=0)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy3jIJSSh2u6",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "This plot shows that the Random Forest has the best R2 score and explained variance values of the algorithms.\n",
        "The RF model also has the lowest:\n",
        "\n",
        "\n",
        "*  mean squared error\n",
        "*  mean absolute error\n",
        "*  mean squared log error\n",
        "*  median absolute error\n",
        "\n",
        "This is the best regression model in terms of accuracy found for our dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxjMgKe4Kxxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhAqeK9nh2u6",
        "colab_type": "text"
      },
      "source": [
        "## Train a Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLBtf1wwh2u7",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can train a Random Forest regression model with all the data we have, assuming that the more data we have the better the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne6Ma5Amh2u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to remove the string 'regresson__' from the names of the best parameters\n",
        "def transform(dict, prefix):\n",
        "    dict_prefix = {key:value for key,value in dict.items() if prefix in key}\n",
        "    return {key.replace(prefix,''):value for key,value in dict_prefix.items()}\n",
        "\n",
        "pipe = make_pipeline(preprocess_pipeline, \n",
        "                     RF_best_params.get('reduce_dim'),\n",
        "                     RandomForestRegressor(n_estimators=10, **transform(RF_best_params, 'regresson__')))\n",
        "\n",
        "final_model =pipe.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZUe5hCRh2u-",
        "colab_type": "text"
      },
      "source": [
        "We can also store this model on disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgQ6cs-gh2u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'final_model.sav'\n",
        "pickle.dump(final_model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}